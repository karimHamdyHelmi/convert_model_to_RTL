# Importance of RTL Mapper Output Files

This document explains the purpose and importance of every file generated by `rtl_mapper.py`.

## Directory Structure

```
rtl_build/
‚îú‚îÄ‚îÄ rtl/              # SystemVerilog RTL modules
‚îú‚îÄ‚îÄ SIM/              # Memory initialization files
‚îú‚îÄ‚îÄ mapping_report.txt # Human-readable mapping summary
‚îî‚îÄ‚îÄ netlist.json      # Machine-readable architecture description
```

---

## üìÅ RTL Modules (`rtl/` directory)

### Core Compute Modules (Copied from Templates)

#### 1. **`mac.sv`** - Multiply-Accumulate Unit
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Fundamental building block for matrix multiplication
- **Function**: Performs `acc = acc + (a √ó b)` in a pipelined fashion
- **Why Critical**: Every fully-connected layer uses multiple MAC units (one per output neuron)
- **Usage**: Instantiated inside `fc_in.sv` for each neuron
- **Modifications**: None (used as-is from template)

#### 2. **`fc_in.sv`** - Fully-Connected Input Layer (MODIFIED)
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Core FC layer computation engine
- **Function**: 
  - Streams input data through MAC units
  - Accumulates dot products for all neurons
  - Adds biases after accumulation
  - **NEW**: Shifts MAC result right by FRAC_BITS before bias addition (fixed-point scaling)
- **Why Critical**: This is the heart of every FC layer
- **Modifications**: Added `FRAC_BITS` parameter and right-shift operation for correct fixed-point math
- **Usage**: Instantiated by each `fc_layer_*.sv` wrapper

#### 3. **`relu_layer.sv`** - ReLU Activation
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê HIGH**
- **Purpose**: Applies ReLU activation function element-wise
- **Function**: `output = (input < 0) ? 0 : input`
- **Why Important**: Required for all ReLU activations in the network
- **Usage**: Instantiated in top module for each ReLU layer
- **Modifications**: None (used as-is from template)

#### 4. **`sat32_to_16.sv`** - Saturation Module
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê HIGH**
- **Purpose**: Clamps 32-bit accumulator values to 16-bit range
- **Function**: Saturates values outside [-32768, 32767] to prevent overflow
- **Why Important**: 
  - FC layers produce 32-bit accumulations
  - Next layer expects 16-bit inputs
  - Prevents data corruption from overflow
- **Usage**: Applied after ReLU to prepare data for next FC layer
- **Generated**: Created by rtl_mapper (not in templates)

---

### Per-Layer Generated Modules

#### 5. **`weight_rom_<layer>.sv`** - Weight ROM Module
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Stores quantized weights for one FC layer
- **Function**: 
  - Reads weights from `.mem` file at simulation time
  - Provides packed weight rows (all neurons for one input feature)
  - Addressable by input feature index
- **Why Critical**: 
  - Contains all learned weights for the layer
  - Without this, the network has no knowledge
  - Must match exact memory layout from `.mem` file
- **Memory Layout**: 
  - Depth: `in_features √ó num_neurons` (one weight per line in .mem)
  - Packed output: `num_neurons √ó weight_width` bits per address
- **Generated**: One per FC layer (e.g., `weight_rom_fc1.sv`, `weight_rom_fc2.sv`)

#### 6. **`bias_rom_<layer>.sv`** - Bias ROM Module
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Stores quantized biases for one FC layer
- **Function**: 
  - Reads biases from `.mem` file at simulation time
  - Provides all biases for the layer in packed format
- **Why Critical**: 
  - Biases are essential for correct neural network behavior
  - Each neuron has one bias value
- **Memory Layout**: 
  - Single entry containing all `num_neurons` biases
  - Packed: `num_neurons √ó acc_width` bits
- **Generated**: One per FC layer (e.g., `bias_rom_fc1.sv`, `bias_rom_fc2.sv`)

#### 7. **`fc_layer_<layer>.sv`** - FC Layer Wrapper
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Complete FC layer implementation combining ROMs and compute
- **Function**: 
  - Instantiates weight ROM, bias ROM, and `fc_in` compute unit
  - Slices packed ROM outputs into individual weights/biases
  - Manages weight ROM addressing based on input stream
  - Exposes clean interface: `valid_in`, `input_data` ‚Üí `fc_out[]`, `valid_out`
- **Why Critical**: 
  - This is the complete, usable FC layer module
  - Top module instantiates these, not the individual components
  - Handles all the complexity of ROM management
- **Generated**: One per FC layer (e.g., `fc_layer_fc1.sv`, `fc_layer_fc2.sv`)

#### 8. **`<model_name>_top.sv`** - Top-Level Module
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Complete neural network implementation
- **Function**: 
  - Instantiates all FC layers and ReLU layers
  - Manages streaming data flow between layers
  - Handles serialization/deserialization (FC outputs vectors, next FC needs scalars)
  - Applies saturation between layers
  - Provides top-level interface: `in_valid`, `in_data` ‚Üí `out_valid`, `out_data[]`
- **Why Critical**: 
  - **This is the main module you synthesize and simulate**
  - Contains the entire network architecture
  - Handles all timing and data movement
  - Without this, you have components but no complete system
- **Complexity**: Most complex generated file - handles:
  - Streaming input (784 cycles for MNIST)
  - FC ‚Üí ReLU ‚Üí Serialize ‚Üí FC pipeline
  - Vector-to-scalar conversion
  - Output vector presentation

#### 9. **`tb_<model_name>_top.sv`** - Testbench (Optional)
**Importance: ‚≠ê‚≠ê‚≠ê MEDIUM**
- **Purpose**: Simple testbench for simulation
- **Function**: 
  - Generates clock and reset
  - Feeds dummy input vectors
  - Monitors output valid and data
  - Basic sanity check that the design works
- **Why Useful**: 
  - Quick verification that synthesis/simulation works
  - Can be extended for more thorough testing
  - Helps debug timing issues
- **Generated**: Only if `--emit-testbench` flag is used

---

## üíæ Memory Files (`SIM/` directory)

### 10. **`<layer>_weights_packed.mem`** - Weight Memory File
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Contains quantized weights in hex format
- **Format**: 
  - One 16-bit hex value per line (4 hex digits)
  - Layout: For each input feature j, all neuron weights W[:, j] in reverse order
  - Total lines: `in_features √ó num_neurons`
- **Why Critical**: 
  - **This is the actual learned knowledge of your network**
  - ROM modules read this at simulation time
  - Must match exact quantization from PyTorch model
  - Wrong values = wrong network behavior
- **Example**: For FC1 (784‚Üí16), contains 784√ó16 = 12,544 weight values
- **Generated**: One per FC layer

### 11. **`<layer>_biases.mem`** - Bias Memory File
**Importance: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê CRITICAL**
- **Purpose**: Contains quantized biases in hex format
- **Format**: 
  - One 16-bit hex value per line (4 hex digits)
  - One bias per neuron
  - Total lines: `num_neurons`
- **Why Critical**: 
  - Biases are essential for correct computation
  - Each neuron needs its bias value
  - Must match PyTorch model exactly
- **Example**: For FC1 (16 neurons), contains 16 bias values
- **Generated**: One per FC layer

---

## üìÑ Documentation Files

### 12. **`mapping_report.txt`** - Human-Readable Report
**Importance: ‚≠ê‚≠ê‚≠ê MEDIUM**
- **Purpose**: Summary of the mapping process
- **Contents**: 
  - Model name and configuration
  - Layer-by-layer breakdown
  - File locations
  - Parameter values (scale, widths, etc.)
- **Why Useful**: 
  - Quick reference for what was generated
  - Helps understand the architecture
  - Useful for debugging
  - Documents the mapping for future reference
- **Format**: Plain text, easy to read

### 13. **`netlist.json`** - Machine-Readable Netlist
**Importance: ‚≠ê‚≠ê‚≠ê MEDIUM**
- **Purpose**: Structured data about the network architecture
- **Contents**: 
  - JSON format with layer information
  - Layer types, dimensions
  - Memory file paths
  - Can be parsed by other tools
- **Why Useful**: 
  - Enables automated tooling
  - Can be used by other scripts/tools
  - Machine-readable format for integration
  - Useful for verification scripts

---

## üîÑ Data Flow Summary

```
Input Image (784 pixels)
    ‚Üì
[rtl/SmallMNISTNet_top.sv]
    ‚Üì
fc_layer_fc1.sv ‚Üí Reads fc1_weights_packed.mem, fc1_biases.mem
    ‚Üì (16 outputs)
relu_layer.sv ‚Üí sat32_to_16.sv
    ‚Üì (serialize to 16 cycles)
fc_layer_fc2.sv ‚Üí Reads fc2_weights_packed.mem, fc2_biases.mem
    ‚Üì (16 outputs)
relu_layer.sv ‚Üí sat32_to_16.sv
    ‚Üì (serialize to 16 cycles)
fc_layer_fc3.sv ‚Üí Reads fc3_weights_packed.mem, fc3_biases.mem
    ‚Üì (10 outputs)
Output Logits
```

---

## üéØ Critical Path Files (Must Have)

These files are absolutely essential for the RTL to work:

1. **`<model_name>_top.sv`** - Main module
2. **`fc_layer_*.sv`** - All FC layer wrappers
3. **`weight_rom_*.sv`** - All weight ROMs
4. **`bias_rom_*.sv`** - All bias ROMs
5. **`fc_in.sv`** - Modified compute engine
6. **`mac.sv`** - MAC unit
7. **`relu_layer.sv`** - ReLU activation
8. **`sat32_to_16.sv`** - Saturation module
9. **`*_weights_packed.mem`** - All weight memory files
10. **`*_biases.mem`** - All bias memory files

---

## üìù Notes

- **Templates vs Generated**: `mac.sv` and `relu_layer.sv` are copied unchanged. `fc_in.sv` is modified to add FRAC_BITS support. Everything else is generated.
- **Memory Files**: Must be in `SIM/` directory relative to where simulation runs (ROM modules use `../SIM/` paths)
- **Fixed-Point Math**: The FRAC_BITS shift in `fc_in.sv` is critical for correct fixed-point computation matching the Python quantized model.
